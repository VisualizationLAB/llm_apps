# Databricks Asset Bundle Configuration for Provider Network Management System
# This configuration file defines how to deploy and manage the provider network
# management application across different Databricks environments

bundle:
  name: provider-network-management
  # Version tracking for deployments
  git:
    origin_url: https://github.com/your-org/provider-network-management
    branch: main

# Variables that can be overridden per environment
variables:
  # Application configuration
  app_name:
    description: "Name of the provider network management application"
    default: "provider-network-mgmt"
  
  # Cluster configuration
  cluster_node_type:
    description: "Node type for compute clusters"
    default: "i3.xlarge"
  
  cluster_min_workers:
    description: "Minimum number of worker nodes"
    default: 1
  
  cluster_max_workers:
    description: "Maximum number of worker nodes" 
    default: 4
  
  # Database and storage
  catalog_name:
    description: "Unity Catalog name for data storage"
    default: "provider_network"
  
  schema_name:
    description: "Database schema name"
    default: "analytics"
  
  # Notification settings
  alert_email:
    description: "Email for job alerts and notifications"
    default: "devops@company.com"
  
  # Security and access
  service_principal_id:
    description: "Service principal for automated operations"
    default: ""

# Resource definitions
resources:
  # Data processing cluster for heavy analytics workloads
  clusters:
    analytics_cluster:
      cluster_name: "${var.app_name}-analytics-${bundle.environment}"
      spark_version: "13.3.x-scala2.12"
      node_type_id: "${var.cluster_node_type}"
      driver_node_type_id: "${var.cluster_node_type}"
      num_workers: 2
      autoscale:
        min_workers: "${var.cluster_min_workers}"
        max_workers: "${var.cluster_max_workers}"
      
      # Spark configuration optimized for network data processing
      spark_conf:
        "spark.sql.adaptive.enabled": "true"
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
        "spark.sql.execution.arrow.pyspark.enabled": "true"
        "spark.databricks.delta.preview.enabled": "true"
      
      # Libraries for network analysis and visualization
      libraries:
        - pypi:
            package: "streamlit==1.28.0"
        - pypi:
            package: "plotly==5.17.0"
        - pypi:
            package: "pandas==2.1.0"
        - pypi:
            package: "numpy==1.24.3"
        - pypi:
            package: "scikit-learn==1.3.0"
        - pypi:
            package: "matplotlib==3.7.2"
        - pypi:
            package: "seaborn==0.12.2"
        - pypi:
            package: "reportlab==4.0.4"
        - pypi:
            package: "networkx==3.1"
        - pypi:
            package: "requests==2.31.0"
      
      # Auto-termination to save costs
      autotermination_minutes: 30
      
      # Enable Unity Catalog
      data_security_mode: "USER_ISOLATION"
      runtime_engine: "PHOTON"

    # Lightweight cluster for Streamlit app hosting
    app_cluster:
      cluster_name: "${var.app_name}-app-${bundle.environment}"
      spark_version: "13.3.x-scala2.12"
      node_type_id: "i3.large"
      driver_node_type_id: "i3.large"
      num_workers: 0  # Single node for app hosting
      
      spark_conf:
        "spark.databricks.cluster.profile": "singleNode"
        "spark.master": "local[*]"
      
      libraries:
        - pypi:
            package: "streamlit==1.28.0"
        - pypi:
            package: "plotly==5.17.0"
        - pypi:
            package: "pandas==2.1.0"
      
      custom_tags:
        "ResourceClass": "SingleNode"
        "Application": "StreamlitApp"
      
      autotermination_minutes: 120

  # Jobs for automated data processing and analysis
  jobs:
    # Daily network performance analysis
    daily_network_analysis:
      name: "${var.app_name}-daily-analysis-${bundle.environment}"
      description: "Daily automated network performance analysis and reporting"
      
      job_clusters:
        - job_cluster_key: "analytics"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "${var.cluster_node_type}"
            num_workers: 2
            libraries:
              - pypi:
                  package: "pandas==2.1.0"
              - pypi:
                  package: "numpy==1.24.3"
              - pypi:
                  package: "plotly==5.17.0"
      
      tasks:
        - task_key: "data_ingestion"
          description: "Ingest provider network data"
          job_cluster_key: "analytics"
          python_wheel_task:
            package_name: "provider_network_mgmt"
            entry_point: "ingest_data"
            parameters:
              - "--source"
              - "/mnt/provider-data/raw/"
              - "--target"
              - "${var.catalog_name}.${var.schema_name}.provider_metrics"
        
        - task_key: "performance_analysis"
          description: "Run network performance analysis"
          depends_on:
            - task_key: "data_ingestion"
          job_cluster_key: "analytics"
          python_wheel_task:
            package_name: "provider_network_mgmt"
            entry_point: "analyze_performance"
            parameters:
              - "--catalog"
              - "${var.catalog_name}"
              - "--schema"
              - "${var.schema_name}"
        
        - task_key: "cost_optimization"
          description: "Run cost optimization analysis"
          depends_on:
            - task_key: "performance_analysis"
          job_cluster_key: "analytics"
          python_wheel_task:
            package_name: "provider_network_mgmt"
            entry_point: "optimize_costs"
            parameters:
              - "--catalog"
              - "${var.catalog_name}"
              - "--schema"
              - "${var.schema_name}"
        
        - task_key: "generate_reports"
          description: "Generate daily reports"
          depends_on:
            - task_key: "cost_optimization"
          job_cluster_key: "analytics"
          python_wheel_task:
            package_name: "provider_network_mgmt"
            entry_point: "generate_daily_report"
            parameters:
              - "--output"
              - "/mnt/reports/daily/"
              - "--email"
              - "${var.alert_email}"
      
      # Schedule for daily execution
      schedule:
        quartz_cron_expression: "0 0 6 * * ?"  # Daily at 6 AM
        timezone_id: "UTC"
        pause_status: "UNPAUSED"
      
      # Email notifications
      email_notifications:
        on_success:
          - "${var.alert_email}"
        on_failure:
          - "${var.alert_email}"
        on_duration_warning_threshold_exceeded:
          - "${var.alert_email}"
      
      timeout_seconds: 7200  # 2 hours timeout
      max_concurrent_runs: 1

    # Weekly comprehensive analysis
    weekly_comprehensive_analysis:
      name: "${var.app_name}-weekly-analysis-${bundle.environment}"
      description: "Weekly comprehensive network analysis with trend detection"
      
      job_clusters:
        - job_cluster_key: "heavy_analytics"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "i3.2xlarge"
            num_workers: 4
            autoscale:
              min_workers: 2
              max_workers: 8
      
      tasks:
        - task_key: "trend_analysis"
          description: "Analyze weekly trends and patterns"
          job_cluster_key: "heavy_analytics"
          python_wheel_task:
            package_name: "provider_network_mgmt"
            entry_point: "analyze_trends"
            parameters:
              - "--period"
              - "weekly"
              - "--catalog"
              - "${var.catalog_name}"
        
        - task_key: "ml_predictions"
          description: "Run ML models for performance predictions"
          depends_on:
            - task_key: "trend_analysis"
          job_cluster_key: "heavy_analytics"
          python_wheel_task:
            package_name: "provider_network_mgmt"
            entry_point: "predict_performance"
            parameters:
              - "--model_path"
              - "/mnt/models/network_performance/"
              - "--forecast_days"
              - "30"
      
      schedule:
        quartz_cron_expression: "0 0 7 ? * SUN"  # Sundays at 7 AM
        timezone_id: "UTC"
      
      email_notifications:
        on_success:
          - "${var.alert_email}"
        on_failure:
          - "${var.alert_email}"

  # Model serving endpoints for real-time predictions
  model_serving_endpoints:
    network_performance_predictor:
      name: "${var.app_name}-performance-predictor-${bundle.environment}"
      config:
        served_models:
          - name: "network-performance-model"
            model_name: "provider_network.models.performance_predictor"
            model_version: "latest"
            workload_size: "Small"
            scale_to_zero_enabled: true
        auto_capture_config:
          catalog_name: "${var.catalog_name}"
          schema_name: "${var.schema_name}"
          table_name_prefix: "inference_logs"

  # Delta Live Tables for real-time data processing
  pipelines:
    network_data_pipeline:
      name: "${var.app_name}-data-pipeline-${bundle.environment}"
      description: "Real-time network data processing pipeline"
      
      # Pipeline configuration
      configuration:
        "catalog": "${var.catalog_name}"
        "schema": "${var.schema_name}"
        "target_database": "${var.catalog_name}.${var.schema_name}"
      
      libraries:
        - notebook:
            path: "./notebooks/dlt_network_pipeline.py"
      
      # Cluster configuration for pipeline
      clusters:
        - label: "default"
          num_workers: 2
          node_type_id: "${var.cluster_node_type}"
          
      # Continuous processing for real-time updates
      continuous: false
      development: false
      
      # Data quality expectations
      expectations:
        "valid_latency": "latency_ms IS NOT NULL AND latency_ms > 0"
        "valid_uptime": "uptime_percentage BETWEEN 0 AND 100"
        "valid_provider": "provider_name IS NOT NULL"

  # Unity Catalog resources
  schemas:
    analytics_schema:
      catalog_name: "${var.catalog_name}"
      name: "${var.schema_name}"
      comment: "Schema for provider network analytics data"
      
      grants:
        - principal: "data_engineers"
          privileges: ["USE", "CREATE_TABLE", "MODIFY"]
        - principal: "data_analysts" 
          privileges: ["USE", "SELECT"]

  # Volumes for file storage
  volumes:
    provider_data:
      catalog_name: "${var.catalog_name}"
      schema_name: "${var.schema_name}"
      name: "provider_data"
      volume_type: "MANAGED"
      comment: "Storage for provider network data files"
    
    reports_output:
      catalog_name: "${var.catalog_name}"
      schema_name: "${var.schema_name}"
      name: "reports"
      volume_type: "MANAGED" 
      comment: "Storage for generated reports and outputs"

# Environment-specific configurations
environments:
  # Development environment
  development:
    mode: development
    default: true
    
    variables:
      cluster_node_type: "i3.large"
      cluster_min_workers: 1
      cluster_max_workers: 2
      alert_email: "dev-team@company.com"
    
    # Override cluster configs for dev
    resources:
      clusters:
        analytics_cluster:
          autotermination_minutes: 15  # Shorter timeout for dev
          num_workers: 1
        
        app_cluster:
          autotermination_minutes: 60

  # Staging environment
  staging:
    mode: production
    
    variables:
      cluster_node_type: "i3.xlarge"
      cluster_min_workers: 1
      cluster_max_workers: 3
      alert_email: "staging-alerts@company.com"
    
    # Workspace configuration
    workspace:
      host: "https://staging.databricks.com"

  # Production environment
  production:
    mode: production
    
    variables:
      cluster_node_type: "i3.2xlarge"
      cluster_min_workers: 2
      cluster_max_workers: 8
      alert_email: "prod-alerts@company.com"
    
    # Production workspace
    workspace:
      host: "https://production.databricks.com"
    
    # Enhanced monitoring and alerting for production
    resources:
      jobs:
        daily_network_analysis:
          email_notifications:
            on_success:
              - "prod-alerts@company.com"
              - "management@company.com"
            on_failure:
              - "prod-alerts@company.com"
              - "on-call@company.com"
          
          # More aggressive timeout for production
          timeout_seconds: 3600
        
        # Production-only monitoring job
        system_health_monitor:
          name: "${var.app_name}-health-monitor-${bundle.environment}"
          description: "Monitor system health and data quality"
          
          tasks:
            - task_key: "data_quality_check"
              existing_cluster_id: "analytics_cluster"
              python_wheel_task:
                package_name: "provider_network_mgmt"
                entry_point: "check_data_quality"
          
          schedule:
            quartz_cron_expression: "0 */2 * * * ?"  # Every 2 hours
            timezone_id: "UTC"

# Include patterns for deployment
include:
  - "agents/**"
  - "utils/**"
  - "streamlit_integration/**"
  - "data/providers.csv"
  - "requirements.txt"
  - "setup.py"
  - "notebooks/**"

# Exclude patterns
exclude:
  - "**/__pycache__/**"
  - "**/.pytest_cache/**"
  - "**/node_modules/**"
  - "**/.git/**"
  - "**/tmp/**"
  - "**/*.log"